---
title: "Answers of StatComp course"
author: "Lief Lv"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Answers of StatComp course}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp20052__ provides the answers of homework for the 'Statistical Computing' course.

# hw1
## Question

Use knitr to produce 3 examples in the book. The 1st example should contain texts and at least one figure. The 2nd example should contains texts and at least one table. The 3rd example
should contain at least a couple of LaTeX formulas.

## Answer

example 1：
This is an example from page 45. It includes a figure.

```{r}
x <- rnorm(10)
y <- rnorm(10)
plot(x,y)
```

example 2：
This is an example includes a table.

```{r}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef
```

example 3:
This is an example contains at least a couple of LaTex formulas.And this formula comes from the paper I am reading.

$$
\Phi(x)=(2 \pi)^{-\frac{1}{2}} \int_{-\infty}^{x} e^{-\frac{1}{2} u^{2}} d u
$$




# hw2
## Question

### 3.3
The Pareto $(a, b)$ distribution has cdf
$$
F(x)=1-\left(\frac{b}{x}\right)^{a}, \quad x \geq b>0, a>0
$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto( 2,2$)$ distribution. Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.


### 3.9
The rescaled Epanechnikov kernel [85] is a symmetric density function
$$
f_{e}(x)=\frac{3}{4}\left(1-x^{2}\right), \quad|x| \leq 1
$$
Devroye and Györfi $[71,$ p. 236$]$ give the following algorithm for simulation from this distribution. Generate idd $U_{1}, U_{2}, U_{3} \sim$ Uniform $(-1,1) .$ If $\left|U_{3}\right| \geq$ $\left|U_{2}\right|$ and $\left|U_{3}\right| \geq\left|U_{1}\right|,$ deliver $U_{2} ;$ otherwise deliver $U_{3} .$ Write a function to generate random variates from $f_{e},$ and construct the histogram density estimate of a large simulated random sample.


### 3.10
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{e}(3.10)$


### 3.13
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf
$$
F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0
$$
(This is an alternative parameterization of the Pareto cdf given in Exercise
3.3.) Generate 1000 random observations from the mixture with $r=4$ and $\beta=2 .$ Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density
curve.






## Answer

### 3.3
It's easy to get$$F^{-1}(u)=\frac{2}{\sqrt{1-u}}$$
I cut the hist in the range from 0 to 20, for the rest of the data is sparse. Here is the simulation. 
```{r}
n <- 1000
u <- runif(n)
x <- 2/sqrt(1-u)#F(x)=1-(2/x)^2
hist(x,prob=TRUE,breaks=100,xlim=c(0,20),main=expression(f(x)==8/x^3))#cut the hist in the range from 0 to 40
y <- seq(0,20,0.1)
lines(y, 8/y^3)


```


### 3.9
Here is the function the problem want.
```{r}
gen <- function(u1,u2,u3)
{
  u <- vector(length = length(u1))
  for(i in 1:length(u1)){
    if(abs(u3[i]) >= abs(u2[i]) & abs(u3[i]) >= abs(u1[i]))
     u[i] = u2[i] else
     u[i] = u3[i]
  }
  return(u)
}

```
Then construct the histogram density estimate of a large simulated random sample.
```{r}
n <- 1000
u1 <- runif(n, min = -1, max = 1)
u2 <- runif(n, min = -1, max = 1)
u3 <- runif(n, min = -1, max = 1)
u <- gen(u1,u2,u3)
hist(u, prob = TRUE)
y <- seq(-1, 1, .01)
lines(y, 3/4*(1-y^2))
```

### 3.10
Let $T_{1}=\left|U_{1}\right|, T_{2}=\left|U_{2}\right|, T_{3}=\left|U_{3}\right|$
Therefore, $T_{1}, T_{2}, T_{3} \sim U(0,1)$
Suppose
$$
\begin{aligned}
Z &=\left\{\begin{array}{ll}
U_{2}, & \text { if } T_{3} \geq \max \left\{T_{1}, T_{2}\right\} \\
U_{3}, & \text { otherwise }
\end{array}\right.\\
Y &=\left\{\begin{array}{ll}
T_{2}, & \text { if } T_{3} \geq \max \left\{T_{1}, T_{2}\right\} \\
T_{3}, & \text { otherwise }
\end{array}\right.
\end{aligned}
$$



# hw3
## Question 5.1

Compute a Monte Carlo estimate of
$$
\int_{0}^{\pi / 3} \sin t d t
$$
and compare your estimate with the exact value of the integral.

## Answer

```{r}
m <- 1e4
x <- runif(m, min=0, max= pi/3)
theta.hat <- mean(sin(x)) * pi/3
print(c(theta.hat,cos(0) - cos(pi/3)))
```



## Question 5.7

Refer to Exercise $5.6 .$ Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6 .

## Answer

By the antithetic variate approach:
```{r}
set.seed(1234)
m <- 1e4/2; x <- runif(m, min=0, max=1)
MC1 <- mean(exp(x)+exp(1-x))/2
print(MC1)
```
By the simple Monte Carlo method:
```{r}
set.seed(1234)
m <- 1e4; x <- runif(m, min=0, max=1)
MC2 <- mean(exp(x))
print(MC2)
```
reduction:
```{r}
set.seed(1234)
m <- 1e4; x1 <- runif(m/2, min=0, max=1)
x2 <- runif(m, min=0, max=1)
print(1-var((exp(x1)+exp(1-x1))/2)/var(exp(x2)))
```
While the theoretical reduction is:
$$
\begin{aligned}
1-\frac{Var \left( \frac{e^u+e^{1-u}}{2} \right)}{Var \left(e^u \right)} &= 1-\frac{Var \left(e^u + e^{1-u} \right)}{4Var(e^u)}\\
&= 1-\frac{Var(e^u)+ Var(e^{1-u})+ 2Cov(e^u,e^{1-u})}{4Var(e^u)}\\
&= 1-\frac{2 \left( \frac{e^2-1}{2}-(e-1)^2 \right)+2 \left(e-(e-1)^2 \right)}{4\left( \frac{e^2-1}{2}-(e-1)^2 \right)}\\
&= 0.9838350
\end{aligned}
$$



# hw4
## Question 5.13

Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling? Explain.


## Answer
I choose two importance functions:
$$f_0(x)=e^{-x}, \quad\quad 0<x<\infty$$
$$f_1(x)=\frac{1}{ \sqrt{2 \pi}} e^{-\frac{x^{2}}{2}}, \quad\quad -\infty<x<\infty.$$

```{r}
m <- 10000
theta.hat <- se <- numeric(2) 
g <- function(x) {
  x^2*exp(-x^2/2)/sqrt(2*pi) * (x>1) }

set.seed(1234)

x <- rexp(m, 1) 
fg <- g(x) / exp(-x) 
theta.hat[1] <- mean(fg) 
se[1] <- sd(fg)

x <- rnorm(m)
fg <- g(x) / (exp(-x^2/2)/sqrt(2*pi))
theta.hat[2] <- mean(fg) 
se[2] <- sd(fg)

rbind(theta.hat, se)
```
So the simulation indicates that $f_0$ produce smallest variance among these two importance functions.


## Question 5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10 .


## Answer
In Example 5.10 our best result was obtained with importance function $f_{3}(x)=$ $e^{-x} /\left(1-e^{-1}\right), 0<x<1 .$ From 10000 replicates we obtained the estimate $\hat{\theta}=0.5257801$ and an estimated standard error $0.0970314 .$ Now divide the interval (0,1) into five subintervals, $(j / 5,(j+1) / 5), j=0,1, \ldots, 4$ Then

```{r}
m <- 10000
N <- 5
T <- matrix(0,N,2)
for(i in 1:N)
{
  u <- runif(m)
  z <- exp((1-i)/5)-exp(-i/5) 
  x <- -log(exp(-0.2*i+0.2)-(u*z)) 
  g <- function(y){z/(1+y^2)*(y>((i-1)/5))*(y<(i/5))}
  d <-g(x)
  T[i,1] <- mean(d)
  T[i,2] <- var(d)
}
est1 <- sum(T[,1]);est1
est2 <- 5*sum(T[,2]);est2

```

## Question 6.4

Suppose that $X_{1}, \ldots, X_{n}$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95 \%$ confidence interval for the parameter $\mu .$ Use a Monte Carlo method to obtain an empirical estimate of the confidence level.


## Answer

$lnx \sim N(\mu,\sigma^2)$
Let $S^2 = \frac{1}{n-1} \sum_{i=1}^{n}\left(lnx_{i}-\overline{lnx}\right)^{2}.$
Then$$\frac{\sqrt{n}(\overline{lnx}-\mu)}{S} \sim t(n)$$
It's easy to get a $95 \%$ confidence interval for the parameter $\mu .$
$$\mu \in\left[u-1.96 * \frac{S}{\sqrt{n}}, \quad u+1.96 * \frac{S}{\sqrt{n}}\right]$$

Then use a Monte Carlo method to obtain an empirical estimate of the confidence level:
Suppose $lnx \sim N(0,1)$
```{r}
n <- 20
alpha <- .05
set.seed(1234)
UCL <- replicate(1000, expr = {x <- rnorm(n, mean = 0, sd = 1)
mean(x) + var(x)*qt(alpha/2, df = n-1)/sqrt(n) })
DCL <- replicate(1000, expr = {x <- rnorm(n, mean = 0, sd = 1)
mean(x) - var(x)*qt(alpha/2, df = n-1)/sqrt(n) })
mean(UCL < 0 & 0 < DCL)

```


## Question 6.5

Suppose a $95 \%$ symmetric $t$ -interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to $0.95 .$ Use a Monte Carlo experiment to estimate the coverage probability of the $t$ -interval for random samples of $\chi^{2}(2)$ data with sample size $n=20 .$ Compare your $t$ -interval results with the simulation results in Example $6.4 .$ (The $t$ -interval should be more robust to departures from normality than the interval for variance.)


## Answer

```{r}
n <- 20
alpha <- .05
set.seed(1234)
UCL <- replicate(1000, expr = {x <- rchisq(n, df=2)
mean(x) + var(x)*qt(alpha/2, df = n-1)/sqrt(n) })
DCL <- replicate(1000, expr = {x <- rchisq(n, df=2)
mean(x) - var(x)*qt(alpha/2, df = n-1)/sqrt(n) })
mean(UCL < 0 & 0 < DCL)
```



# hw5
## Question 6.7

Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu) ?$


## Answer

For $\operatorname{Beta}(\alpha, \alpha)$
```{r}
sk <- function(x) { 
  #computes the sample skewness coeff. 
  xbar <- mean(x) 
  m3 <- mean((x - xbar)^3) 
  m2 <- mean((x - xbar)^2) 
  return( m3 / m2^1.5 )
}

pwr_beta = function(a){
 alpha = 0.1
 n = 20
 m = 1e4
 N = length(a)
 pwr = numeric(N)
 cv = qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
 
 for (j in 1:N) { 
  sktests = numeric(m)
  for (i in 1:m) { 
   x = rbeta(n, a[j], a[j])
   sktests[i] = as.integer(abs(sk(x))>= cv)
  }
  pwr[j] = mean(sktests)
 }
 se = sqrt(pwr * (1-pwr) / m) 
 return(list(pwr = pwr,se = se))
}

 a = c(seq(0,1,0.1),seq(1,20,1),seq(20,100,10))
 pwr = pwr_beta(a)$pwr
 # plot the power
 se = pwr_beta(a)$se
 plot(a, pwr, type = "b", xlab = "a", ylab = "pwr", pch=16)
 abline(h = 0.1, lty = 2)
 lines(a, pwr+se, lty = 4)
 lines(a, pwr-se, lty = 4)


```

The power of the skewness test of normality against symmetric Beta(a,a) distribution is under 0.1. With the increase of x, the power aproximates to 0.1.

```{r,beta}

# t(v)
pwr_t = function(v){
 
 alpha = 0.1
 n = 20
 m = 1e3
 N = length(v)
 pwr = numeric(N)
 cv = qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
 
 for (j in 1:N) { 
  sktests = numeric(m)
  for (i in 1:m) { 
   x = rt(n,v[j])
   sktests[i] = as.integer(abs(sk(x))>= cv)
  }
  pwr[j] = mean(sktests)
 }
 se = sqrt(pwr*(1-pwr) / m) 
  return(list(pwr = pwr,se = se))
}

v = seq(1,20)
pwr = pwr_t(v)$pwr
se = pwr_t(v)$se
# plot the power
plot(v, pwr, type = "b", xlab = "v", ylab = "pwr", ylim = c(0,1),pch=16)
abline(h = 0.1, lty = 2)
lines(v, pwr+se, lty = 4)
lines(v, pwr-se, lty = 4)

```

The results are different for heavy-tailed symmetric alternatives. 


## Question 6.8

Refer to Example $6.16 .$ Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{\alpha} \doteq 0.055 .$ Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.


## Answer
```{r}
count5test <- function(x, y) {
        X <- x - mean(x)
        Y <- y - mean(y)
        outx <- sum(X > max(Y)) + sum(X < min(Y))
        outy <- sum(Y > max(X)) + sum(Y < min(X))
        return(as.integer(max(c(outx, outy)) > 5))
}
set.seed(12345)
alpha.hat <- 0.055
n <- c(10, 20, 50, 100, 500, 1000)
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
m <- 1e4
result <- matrix(0, length(n), 2)
for (i in 1:length(n)){
  ni <- n[i]
  tests <- replicate(m, expr={
    x <- rnorm(ni, mu1, sigma1)
    y <- rnorm(ni, mu2, sigma2)
    Fp <- var.test(x, y)$p.value
    Ftest <- as.integer(Fp <= alpha.hat)
    c(count5test(x, y), Ftest)
    })
  result[i, ] <- rowMeans(tests)
}
data.frame(n=n, C5=result[, 1], Fp=result[, 2])

```
The F-test for equal variance is more powerful in this case.


## Question 6.C

Repeat Examples 6.8 and 6.10 for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}
$$
Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.


## Answer
We first repeat Example 6.8 which evaluate t1e rate of Mardia’s multivariate skewness test. In our simulation we generate variables following $N(\mu,\Sigma)$, where:
\[\mu=(0,0,0)^{T} , \Sigma=\left( \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right).\]
```{r}
library(MASS)
Mardia<-function(mydata){
  n=nrow(mydata)
  c=ncol(mydata)
  central<-mydata
  for(i in 1:c){
    central[,i]<-mydata[,i]-mean(mydata[,i])
  }
  sigmah<-t(central)%*%central/n
  a<-central%*%solve(sigmah)%*%t(central)
  b<-sum(colSums(a^{3}))/(n*n)
  test<-n*b/6
  chi<-qchisq(0.95,c*(c+1)*(c+2)/6)
  as.integer(test>chi)
}

set.seed(1234)
mu <- c(0,0,0)
sigma <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
m=1000
n<-c(10, 20, 30, 50, 100, 500)
#m: number of replicates; n: sample size
a=numeric(length(n))
for(i in 1:length(n)){
  a[i]=mean(replicate(m, expr={
    mydata <- mvrnorm(n[i],mu,sigma) 
    Mardia(mydata)
  }))
}
```

We calculate the t1e when the sample size is 10, 20, 30, 50, 100, 500: 
```{r}
print(a)
```
From the result we can see that t1e rate is close to 0.05 after the sample size is large than 50.


We further repeat Example 6.8 which evaluate the power of Mardia’s multivariate skewness test under distribution $(1-\epsilon)N(\mu_{1},\Sigma_{1})+\epsilon N(\mu_{2},\Sigma_{2})$, where:
\[\mu_{1}=\mu_{2}=(0,0,0)^{T}, \Sigma_{1}=\left( \begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{array} \right)
\Sigma_{2}=\left( \begin{array}{ccc}
100 & 0 & 0 \\
0 & 100 & 0 \\
0 & 0 & 100 \end{array} \right).\]
```{r}
library(MASS)
set.seed(7912)
set.seed(7912)
mu1 <- mu2 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
sigma2 <- matrix(c(100,0,0,0,100,0,0,0,100),nrow=3,ncol=3)
sigma=list(sigma1,sigma2)
m=1000
n=50
#m: number of replicates; n: sample size
epsilon <- c(seq(0, .06, .01), seq(.1, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
    index=sample(c(1, 2), replace = TRUE, size = n, prob = c(1-e, e))
    mydata<-matrix(0,nrow=n,ncol=3)
    for(t in 1:n){
      if(index[t]==1) mydata[t,]=mvrnorm(1,mu1,sigma1) 
      else mydata[t,]=mvrnorm(1,mu2,sigma2)
    }
    sktests[i] <- Mardia(mydata)
  }
  pwr[j] <- mean(sktests)
}
plot(epsilon, pwr, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

When $\epsilon=0$ or $\epsilon=1$ the distribution is multinormal, when $0\leq \epsilon \leq 1$ the
empirical power of the test is greater than 0.05 and highest(close to 1) when $0.1\leq \epsilon \leq 0.3$.


## Discussion

If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

- What is the corresponding hypothesis test problem?
- What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?
- What information is needed to test your hypothesis?

## Answer

(1) Denote the powers of two methods as $pwr_{1}$ and $pwr_{2}$, then the corresponding hypothesis test problem is:
$$H_{0}: pwr_{1}=pwr_{2} \leftrightarrow H_{1}: pwr_{1}\not=pwr_{2}.$$
(2) As the p-value of two methods for the same sample is not independent, we can not apply the two-sample t-test. For the z-test and paired-t test, when the sample size is large, we have the mean value of significance test follows a normal distribution, thus these two methods can be used in the approximate level. McNemar test is good at dealing with this case as it doesn't need to know the distribution.

(3) For these test, what we already know is the number of experiments and the value of power(the probability that we reject the null hypothesis correctly). To conduct this test, we also need to know the significance of both methods for each sample. 







# hw6
## Question 7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2 .

## Answer

```{r}
library(bootstrap)
n <- nrow(law)
y <- law$LSAT
z <- law$GPA
theta.hat <- cor(y,z)
theta.jack <- numeric(n)
for (i in 1:n)  theta.jack[i] <- cor(y[-i],z[-i])
bias.jack <- (n - 1) * (mean(theta.jack) - theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))

bias.jack
se.jack

```

## Question 7.5

Refer to Exercise $7.4 .$ Compute $95 \%$ bootstrap confidence intervals for the mean time between failures $1 / \lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer

```{r}
library(boot) #for boot and boot.ci
data(aircondit, package = "bootstrap")
theta.boot <- function(dat, ind) {
  #function to compute the statistic
  mean(dat[ind, 1])
}
boot.obj <- boot(aircondit, statistic = theta.boot, R = 2000)
print(boot.obj)
print(boot.ci(boot.obj, type = c("norm", "basic", "perc", "bca")))

```
The output are quite different. Because different output uses different method.


## Question 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of \hat{\theta}.

## Answer
```{r}
set.seed(123)
library(bootstrap)
library(boot)

lambda_hat <- eigen(cov(scor))$values
theta_hat <- lambda_hat[1] / sum(lambda_hat)
B <- 200 # number of bootstrap samples
n <- nrow(scor) # number of rows (data size)


# Jackknife
theta_j <- rep(0, n) 
for (i in 1:n) {
  x <- scor [-i,]
  lambda <- eigen(cov(x))$values
  theta_j[i] <- lambda[1] / sum(lambda)
# the i-th entry of theta_j is the i-th "leave-one-out" estimation of theta
}
bias_jack <- (n - 1) * (mean(theta_j) - theta_hat) 
# the estimated bias of theta_hat, using jackknife
se_jack <- (n - 1) * sqrt(var(theta_j) / n)
# the estimated se of theta_hat, using jackknife

# print the answers 
bias_jack
se_jack
```


## Question 7.11

In Example 7.18, leave-one-out ($n$-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer

```{r}
library(DAAG); attach(ironslag)

n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)


for (k in 1:n) {
  if(k < n) 
    i <- k+1  else  i <- 1
  
  y <- magnetic[c(-k,-i)]
  x <- chemical[c(-k,-i)]
 
  J1 <- lm(y ~ x)
  yhat1k <- J1$coef[1] + J1$coef[2] * chemical[k]
  yhat1i <- J1$coef[1] + J1$coef[2] * chemical[i]
  e1[k] <- (magnetic[k] + magnetic[i]- yhat1k -yhat1i)/2
  
  J2 <- lm(y ~ x + I(x^2))
  yhat2k <- J2$coef[1] + J2$coef[2] * chemical[k] + J2$coef[3] * chemical[k]^2
  yhat2i <- J2$coef[1] + J2$coef[2] * chemical[i] + J2$coef[3] * chemical[i]^2
  e2[k] <- (magnetic[i] + magnetic[k] - yhat2i -yhat2k)/2
    
  J3 <- lm(log(y) ~ x)
  logyhat3k <- J3$coef[1] + J3$coef[2] * chemical[k]
  logyhat3i <- J3$coef[1] + J3$coef[2] * chemical[i]
  yhat3k <- exp(logyhat3k)
  yhat3i <- exp(logyhat3i)
  e3[k] <- (magnetic[k] + magnetic[i] - yhat3k - yhat3i)/2
    
  J4 <- lm(log(y) ~ log(x))
  logyhat4k <- J4$coef[1] + J4$coef[2] * log(chemical[k])
  logyhat4i <- J4$coef[1] + J4$coef[2] * log(chemical[i])
  yhat4k <- exp(logyhat4k)
  yhat4i <- exp(logyhat4i)
  e4[k] <- (magnetic[k] + magnetic[i] - yhat4k - yhat4i)/2
}

c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```
Compare with the leave-one-out ($n$-fold) cross validation, with is
```{r, include=FALSE}
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4[k] <- magnetic[k] - yhat4
}
```

```{r}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```
According to the prediction error criterion, Model 2, the quadratic model, would be the best fit for the data.




# hw7
## Question 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer

```{r}
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y)) 
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer((outx > 4)||(outy > 7)))
}

n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0 
sigma1 <- sigma2 <- 1

x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean 
y <- y - mean(y)
  
R <- 999
z <- c(x, y)
K <- 1:50
D <- numeric(R)

D0 <- count5test(x, y) 
for (i in 1:R) {
   #generate indices k for the first sample
  k <- sample(K, size = 20, replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  D[i] <- count5test(x1, y1) 
}

mean(D)


```

## Question

Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.

$\blacktriangleright$ Unequal variances and equal expectations

$\blacktriangleright$ Unequal variances and unequal expectations

$\blacktriangleright$ Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)

$\blacktriangleright$ Unbalanced samples (say, 1 case versus 10 controls)

$\blacktriangleright$ Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

## Answer

define the functions
```{r}
library(RANN)
library(boot)
library(Ball)
library(energy)

m <- 1e3

Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column? 
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}

eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R, sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}

p.values <- matrix(NA,m,3)

alpha <- 0.1
```

Unequal variances and equal expectations
```{r}
m <- 1e3; k<-3; p<-2; mu <- 0; set.seed(12345)
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)

for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1.5),ncol=p);
  y <- cbind(rnorm(n2),rnorm(n2,mean=mu));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}

pow <- colMeans(p.values<alpha)
pow
```

Unequal variances and unequal expectations
```{r}
m <- 1e3; k<-3; p<-2; mu <- 0.3; set.seed(12345)
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)

for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1.5),ncol=p);
  y <- cbind(rnorm(n2),rnorm(n2,mean=mu));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}

pow <- colMeans(p.values<alpha)
pow
```
Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
```{r}


m <- 1e3; k<-3; p<-2; mu <- 0.3; set.seed(12345)
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)


for(i in 1:m){
  x <- matrix(rt(n1*p,df=1),ncol=p);
  y <- cbind(rnorm(n2),rnorm(n2));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}

pow <- colMeans(p.values<alpha)
pow
```
Unbalanced samples (say, 1 case versus 10 controls)
```{r}
m <- 1e3; k<-3; p<-2; mu <- 0.3; set.seed(12345)
n1 <- 50; n2 <- 100; R<-999; n <- n1+n2; N = c(n1,n2)

for(i in 1:m){
  x <- matrix(rnorm(n1*p,0,1.5),ncol=p);
  y <- cbind(rnorm(n2),rnorm(n2,mean=mu));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=999,seed=i*12345)$p.value
}

pow <- colMeans(p.values<alpha)
pow
```









# hw8
## Question 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

## Answer

```{r}
la <- function(x){
  return(0.5 * exp(-abs(x)))
}

rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N) 
  x[1] <- x0 
  u <- runif(N) 
  k<-0
  for (i in 2:N) { 
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (la(y) / la(x[i-1])))
      x[i] <- y else { 
        x[i] <- x[i-1]
        k<-k+1
      } 
    }
return(list(x=x, k=k))
}

N <- 2000 
sigma <- c(.05, .5, 2, 16)
x0 <- 25
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected

par(mfrow=c(1.5,1.5)) #display 4 graphs together

rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) { 
  plot(rw[,j], type="l",
    xlab=bquote(sigma == .(round(sigma[j],3))),
    ylab="X", ylim=range(rw[,j]))
   
}
par(mfrow=c(1,1)) #reset to default



print(c((N-rw1$k)/N, (N-rw2$k)/N, (N-rw3$k)/N, (N-rw4$k)/N))



```

## Question 9.4(Gelman-Rubin method )

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R} < 1.2$


## Answer

```{r}
la <- function(x){
  return(0.5 * exp(-abs(x)))
}

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}

normal.chain <- function(sigma, N, X1) {
  #generates a Metropolis chain for Normal(0,1)
  #with Normal(X[t], sigma) proposal distribution
  #and starting value X1
  x <- rep(0, N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma) #candidate point
    r1 <- la(y) * dnorm(xt, y, sigma)
    r2 <- la(xt) * dnorm(y, xt, sigma)
    r <- r1 / r2
    if (u[i] <= r) x[i] <- y else
      x[i] <- xt
  }
return(x)
}

sigma <- .2 #parameter of proposal distribution
k <- 4 #number of chains to generate
n <- 15000 #length of chains
b <- 1000 #burn-in length

#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)

#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- normal.chain(sigma, n, x0[i])

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

#plot psi for the four chains
par(mfrow=c(1.5,1.5))
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",
    xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1)) #restore default

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R",ylim = c(0,12))
abline(h=1.2, lty=2)
```

## Question 11.4

Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves
$$
S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^{2}(k-1)}{k-a^{2}}}\right)
$$
and
$$
S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^{2} k}{k+1-a^{2}}}\right)
$$
for k = 4 : 25, 100, 500, 1000, where t(k) is a Student t random variable with
k degrees of freedom. (These intersection points determine the critical values
for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

## Answer

```{r}
f <- function(a) {
  pt(sqrt(k*a^2 / (k+1-a^2)), k) - pt(sqrt((k-1)*a^2 / (k-a^2)), k-1)
}

k <- 4
solution <- uniroot(f,c(0.1,sqrt(k)))
round(unlist(solution),5)

k <- 25
solution <- uniroot(f,c(0.1,sqrt(k)))
round(unlist(solution),5)

k <- 100
solution <- uniroot(f,c(0.1,sqrt(k)))
round(unlist(solution),5)

k <- 500
solution <- uniroot(f,c(0.1,sqrt(k)-1))
round(unlist(solution),5)

k <- 1000
solution <- uniroot(f,c(0.1,sqrt(k)-1))
round(unlist(solution),5)
```






# hw9
## Question 1

A-B-O blood type problem

Let the three alleles be $A, B,$ and $O$. 

\begin{tabular}{l|l|l|l|l|l|l|l}
\hline Genotype & $\mathrm{AA}$ & $\mathrm{BB}$ & $\mathrm{OO}$ & $\mathrm{AO}$ & $\mathrm{BO}$ & $\mathrm{AB}$ & Sum \\
\hline Frequency & $\mathrm{p}^{-} 2$ & $\mathrm{q}^{-2}$ & $\mathrm{r}^{\wedge} 2$ & $2 \mathrm{pr}$ & $2 \mathrm{qr}$ & $2 \mathrm{pq}$ & 1 \\
\hline Count & $\mathrm{nAA}$ & $\mathrm{nBB}$ & $\mathrm{nOO}$ & $\mathrm{nAO}$ & $\mathrm{nBO}$ & $\mathrm{nAB}$ & $\mathrm{n}$ \\
\hline
\end{tabular} 

Observed data: $n_{A \cdot}=n_{A A}+n_{A O}=444($ A-type $),$
$n_{B}=n_{B B}+n_{B O}=132($ B-type $), n_{O O}=361($ O-type $)$ $n_{A B}=63(\mathrm{AB}-\mathrm{type})$

Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{A A}$ and $n_{B B}$ ).

Record the values of $p$ and $q$ that maximize the conditional likelihood in each EM steps, calculate the corresponding log-maximum likelihood values (for observed data), are they increasing?

## Answer

The complete data likelihood is:
$$
l\left(p,q \mid n_{A A}, n_{A O}, n_{B B}, n_{B O}, n_{O O}, n_{A B}\right)=n_{A A} l n p^{2}+n_{B B} l n q^{2}+n_{O O} l n r^{2}+n_{A O} \ln 2 p r+n_{B O} \ln 2 q r+n_{A B} \ln p q
$$
Then,
$$n_{A A} \mid n_{A \cdot}, n_{B}, n_{O O}, n_{A B} \sim B\left(n_{A}, \frac{p^{2}}{p^{2}+2 p r} \right)$$
$$n_{A O} \mid n_{A \cdot}, n_{B}, n_{O O}, n_{A B} \sim B\left(n_{A \cdot}, \frac{2 p r}{p^{2}+2 p r}\right)$$
$$n_{B B} \mid n_{A \cdot}, n_{B}, n_{O O}, n_{A B} \sim B\left(n_{B \cdot}, \frac{q^{2}}{q^{2}+2 q r}\right)$$
$$n_{B O} \mid n_{A,}, n_{B}, n_{O O}, n_{A B} \sim B\left(n_{B \cdot}, \frac{2 q r}{q^{2}+2 q r}\right)$$
By,
$$
\frac{\partial Q}{\partial p} =\frac{2 n_{A A}+n_{A O}+n_{A B}}{p}-\frac{2 n_{O O}+n_{A O}+n_{B O}}{r}$$
$$\frac{\partial Q}{\partial q} =\frac{2 n_{B B}+n_{B O}+n_{A B}}{q}-\frac{2 n_{O O}+n_{A O}+n_{B O}}{r}$$
Solve the equations above and we can get

$$p =\frac{2 n_{A A}+n_{A O}+n_{A B}}{2 n}$$
$$q =\frac{2 n_{B B}+n_{B O}+n_{A B}}{2 n}$$

```{r}
n0<-c(444,132,63,361) # observed data,n_A,n_B,n_AB,n_OO
pq<-c(1/3,1/3)

em<-function(p,n0){
  n<-sum(n0)
  nA<-n0[1]
  nB<-n0[2]
  nAB<-n0[3]
  nOO<-n0[4]
 
  At<-Bt<-Ot<-Q <- numeric(0)
  At[1]<-p[1]
  Bt[1]<-p[2]
  Ot[1]<-1-p[1]-p[2]
  Q[1] <- 0
  for(i in 2:20){
    A.old<-At[i-1]
    B.old<-Bt[i-1]
    O.old<-Ot[i-1]
  
    nAA <- nA * A.old^2/(A.old^2+2*A.old*O.old)
    nAO <- nA * 2*A.old*O.old/(A.old^2+2*A.old*O.old)
    nBB <- nB * B.old^2/(B.old^2+2*B.old*O.old)
    nBO <- nB * 2*B.old*O.old/(B.old^2+2*B.old*O.old)
  
    At[i]<-(2*nAA+nAO+nAB)/(2*n)
    Bt[i]<-(2*nBB+nBO+nAB)/(2*n)
    Ot[i]<-(2*nOO+nAO+nBO)/(2*n)
    Q[i] <- 2*nAA*log(At[i])+2*nBB*log(Bt[i])+2*nOO*log(Ot[i])+
      nAO*log(2*At[i]*Ot[i])+nBO*log(2*Bt[i]*Ot[i])+nAB*log(2*At[i]*Bt[i])
  }
  return(list(At=At,Bt=Bt,Ot=Ot,Q=Q))
}

em(pq,n0)

```
It is increasing.

## Question 2
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
```

## Answer

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

mpg <- mtcars$mpg
disp <- mtcars$disp
wt <- mtcars$wt

lapply(formulas, lm)

out <- vector("list", length(formulas))
for (i in seq_along(formulas)) {
  out[[i]] <- lm(formulas[[i]])
}
out
```



## Question 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
```{r}
trials <- replicate( 
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```
Extra challenge: get rid of the anonymous function by using [ $[$ directly.


## Answer
```{r}
sapply(trials, function(x) x$p.value)

```



## Question 4
Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?


## Answer
```{r}
f_0 <- function(data,f,type){
  m <- Map(f,data)
  vapply(m, function(x) x ,type)
}


```









# hw10
## Question 1

Write an Rcpp function for Exercise 9.4 (page 277, Statistical Computing with R).

## Answer
```{r,eval=FALSE}
#include <Rcpp.h>
using namespace Rcpp;
//[[Rcpp::export]]
float lap_f(int x) {
  float lap = exp(-abs(x));
  return lap;
}
// [[Rcpp::export]]
List rw_MetropolisC(double sigma, double x0, int N) {
  NumericVector x(N);
  NumericVector u(N);
  x[0] = x0;
  u = runif(N);
  int k = 0;
  for (int i = 1; i < N; i++) {
    double y;
    y = rnorm(1,x[i-1],sigma)[0];
    if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) 
      x[i] = y;
    else {
      x[i] = x[i-1];
      k++;
    }
  }
  List out;
  out["x"]=x;
  out["k"]=k;
  return out;
  //return x;
}
```

## Question 2

Compare the corresponding generated random numbers with those by the R function you wrote before using the function
“qqplot”.

## Answer

```{r}
set.seed(123)

lap_f = function(x) exp(-abs(x))

rw_MetropolisR = function(sigma, x0, N){
 x = numeric(N)
 x[1] = x0
 u = runif(N)
 k = 0
 for (i in 2:N) {
  y = rnorm(1, x[i-1], sigma)
  if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
  else {
  x[i] = x[i-1]
  k = k+1
  }
 }
 return(list(x = x, k = k))
}

library(Rcpp)
sourceCpp("../src/rw_MetropolisC.cpp")
N = 2000
sigma = 0.5
x0 = 25
rwC = rw_MetropolisC(sigma,x0,N)
rwR = rw_MetropolisR(sigma,x0,N)

qqplot(rwC$x,rwR$x)
```


## Question 3

Campare the computation time of the two functions with the function “microbenchmark”.

## Answer

```{r}
library(microbenchmark)
ts <- microbenchmark(rwC = rw_MetropolisC(sigma,x0,N),
rwR = rw_MetropolisR(sigma,x0,N)) 
summary(ts)[,c(1,3,5,6)]

```

## Comment

It can be easily seen that Rcpp is much better.
